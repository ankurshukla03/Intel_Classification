{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL model for Intel Classification\n",
    "This is from one of the competition in Analytics Vidhya. Check the details [here](https://datahack.analyticsvidhya.com/contest/practice-problem-intel-scene-classification-challe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from os import path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras import layers\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, Lambda\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils,Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler,EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10051445373718713643\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3169819033\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1678757932215302555\n",
      "physical_device_desc: \"device: 0, name: Quadro P1000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333) \n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'original_dataset/'\n",
    "test = 'test_WyRytb0.csv'\n",
    "train_dir = 'train/'\n",
    "test_dir = 'test/'\n",
    "labels = 'train.csv'\n",
    "sample = 'sample_submission.csv'\n",
    "IMG_SIZE = 150\n",
    "batch_size = 4\n",
    "train_split = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name  label\n",
       "0      0.jpg      0\n",
       "1      1.jpg      4\n",
       "2      2.jpg      5\n",
       "3      4.jpg      0\n",
       "4      7.jpg      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the labels for the training data\n",
    "y_train = pd.read_csv(labels)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name\n",
       "0      3.jpg\n",
       "1      5.jpg\n",
       "2      6.jpg\n",
       "3     11.jpg\n",
       "4     14.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help in preparing the images for test data\n",
    "y_test = pd.read_csv(test)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2628, 2745, 2957, 3037, 2784, 2883]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we have {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }\n",
    "real_label = ['buildings','forest','glacier','mountain','sea','street']\n",
    "frequency = []\n",
    "freq_label = y_train['label'].value_counts()\n",
    "for i in range(len(freq_label)):\n",
    "    frequency.append(freq_label[i])\n",
    "\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label will be like one hot encoding\n",
    "def label_count(image_name,label):\n",
    "    real_label = ['buildings','forest','glacier','mountain','sea','street']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(real_label,frequency)\n",
    "#plt.xticks(real_label,frequency)\n",
    "plt.ylabel('Number of example')\n",
    "plt.title('Distribution of label in training data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation path\n",
    "Reading all the images and saving the path given for the training in train_files and then splitting it into training and validation size. I have used 85-15 for splitting.\n",
    "### Only run once below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy the image to train/val folder\n",
    "#tqdm is a good way for presentation\n",
    "import shutil\n",
    "def copy_image(list_image,des_folder):\n",
    "    for img in list_image['image_name']:\n",
    "        path = os.path.join(images_dir,img)\n",
    "        try:\n",
    "            shutil.copy(path,des_folder)\n",
    "        except shutil.Error as e:\n",
    "            print('Error: %s' % e)\n",
    "        except IOError as e:\n",
    "            print('Error: %s' %e.strerror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "def load_img_paths(target,list_images):\n",
    "    '''\n",
    "    Retrieve the full path of all images given for training the network\n",
    "    '''\n",
    "    path = []\n",
    "    for img in list_images['image_name']:\n",
    "        path.append(glob.glob(target +img))\n",
    "    return path\n",
    "\n",
    "train_files = pd.DataFrame(load_img_paths(train_dir,y_train))\n",
    "\n",
    "train_paths = []\n",
    "valid_paths = []\n",
    "\n",
    "if train_split <= 1:\n",
    "    frac = int(train_files.shape[0] * train_split)\n",
    "    train_paths = train_files[:frac][0].tolist()\n",
    "    valid_paths = train_files[frac:][0].tolist()\n",
    "else:\n",
    "    valid_frac = int(train_split * valid_split)\n",
    "    train_paths = train_files[:train_split][0].values.tolist()\n",
    "    valid_paths = train_files[train_split:train_split+valid_frac][0].values.tolist()\n",
    "\n",
    "assert(len(train_paths) > 0)\n",
    "print('Training images:    %d' % len(train_paths))\n",
    "print('Validation images: %d' % len(valid_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareImages(data, m):\n",
    "    print(\"Preparing images\")\n",
    "    X_train = np.zeros((m, 150, 150, 3))\n",
    "    count = 0\n",
    "    \n",
    "    for fig in data['image_name']:\n",
    "        #load images into images of size 150*150(original size)\n",
    "        #src = cv2.imread(\"train/\"+fig,)\n",
    "        #img = cv2.cvtColor(src,cv2.COLOR_BGR2RGB)\n",
    "        img = image.load_img(\"train/\"+fig, target_size=(150, 150, 3))\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        X_train[count] = x\n",
    "        count += 1\n",
    "        if(count%1000==0):\n",
    "            print('images done :',count)\n",
    "    np.save('traindata_channel_first.npy',X_train)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you already have npy file load that\n",
    "#X_train = prepareImages(y_train,y_train.shape[0])\n",
    "X_train = np.load('traindata.npy')\n",
    "X_train /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17034, 150, 150, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17034, 3, 150, 150)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],3,150,150)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "random_seed = 7\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Output into one hot code\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1 and as this is a multi classification problem so we can convert the output class values into one-hot format which is simply a binary matrix, i.e.\n",
    "\n",
    "value 0 will be converted to one-hot format as [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "value 1 will be converted to one-hot format as [0, 1, 0, 0, 0, 0, 0, 0, 0] etc\n",
    "\n",
    "Here we have {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode outputs'\n",
    "y_train_label = np_utils.to_categorical(y_train['label'])\n",
    "num_classes = y_train_label.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data in Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14478, 150, 150, 3) (14478, 6) (2556, 150, 150, 3) (2556, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split in Training set and Validation set\n",
    "x_train2, x_val, y_train2, y_val = train_test_split(X_train, y_train_label, test_size = 0.15, random_state=random_seed)\n",
    "print (x_train2.shape, y_train2.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "I am using vgg16 like architecture for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(layers, model, num_filters):\n",
    "    \"\"\"\n",
    "    Create a layered Conv/Pooling block\n",
    "    \"\"\"\n",
    "    for i in range(layers):\n",
    "        model.add(Conv2D(num_filters, (3, 3), activation='relu', padding='same')) # 3x3 filter size \n",
    "        \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), data_format='channels_first'))\n",
    "\n",
    "def FCBlock(model, size=2048):\n",
    "    \"\"\"\n",
    "    Fully connected block with ReLU and dropout\n",
    "    \"\"\"\n",
    "    model.add(Dense(size, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "def my_VGG16(input_shape):\n",
    "    \"\"\"\n",
    "    Implement VGG16 architecture\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x : x, input_shape=input_shape))\n",
    "    \n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(2, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    #ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #FCBlock(model)\n",
    "    FCBlock(model)\n",
    "\n",
    "    model.add(Dense(6, activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 3, 150, 150)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 150, 64)        86464     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 150, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 75, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 75, 128)        36992     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 75, 128)        147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 37, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 37, 256)        147712    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 37, 256)        590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 18, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 18, 512)        590336    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 18, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 18, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              14157824  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 20,525,830\n",
      "Trainable params: 20,525,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "im_shape = (3,IMG_SIZE,IMG_SIZE)\n",
    "# Then create the corresponding model \n",
    "my_model = my_VGG16(im_shape)\n",
    "my_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "We will use data augmentation for increasing our dataset and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        data_format='channels_first'\n",
    "        )  # randomly flip simages\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "    \n",
    "# descriptive weight file naming\n",
    "checkpointer = ModelCheckpoint(filepath=('vgg16.h5'), \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [annealer,early_stopping,checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "steps_per_epoch = X_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15330 samples, validate on 1704 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[{{node _arg_lambda_1_input_0_0/_197}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1483__arg_lambda_1_input_0_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node metrics/acc/Mean_1/_231}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1502_metrics/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dcde737fcfb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node _arg_lambda_1_input_0_0/_197}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_1483__arg_lambda_1_input_0_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[{{node metrics/acc/Mean_1/_231}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1502_metrics/acc/Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "hist = my_model.fit(X_train,y_train_label,\n",
    "    validation_split=0.10,\n",
    "    epochs=50,\n",
    "    verbose=2,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    "    validation_steps = 600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Increase data using more data augmentation and also use binary__crossentropy in place of categorical for this multi label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buildings    0\n",
       "Forest       0\n",
       "Glacier      0\n",
       "Mountain     0\n",
       "Sea          1\n",
       "Street       0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_map[path.split(train_paths[1])[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights('model1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Prepare the test Images and make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((X_test.shape[0],1))\n",
    "result = model1.predict(X_test)\n",
    "result = np.argmax(result,axis=1)\n",
    "result = pd.Series(result,name='label')\n",
    "submission = pd.concat([y_test,result],axis = 1)\n",
    "submission.to_csv(\"first_try.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
