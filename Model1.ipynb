{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL model for Intel Classification\n",
    "This is from one of the competition in Analytics Vidhya. Check the details [here](https://datahack.analyticsvidhya.com/contest/practice-problem-intel-scene-classification-challe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from keras import layers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = 'train/'\n",
    "test = 'test_WyRytb0.csv'\n",
    "labels = 'train.csv'\n",
    "sample = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name  label\n",
       "0      0.jpg      0\n",
       "1      1.jpg      4\n",
       "2      2.jpg      5\n",
       "3      4.jpg      0\n",
       "4      7.jpg      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the labels for the training data\n",
    "y_train = pd.read_csv(labels)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7301, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help in preparing the images for test data\n",
    "y_test = pd.read_csv(test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Images for the model\n",
    "We will process the image and make it according to our input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareImages(data, m):\n",
    "    print(\"Preparing images\")\n",
    "    X_train = np.zeros((m, 150, 150, 3))\n",
    "    count = 0\n",
    "    \n",
    "    for fig in data['image_name']:\n",
    "        #load images into images of size 150*150(original size)\n",
    "        #src = cv2.imread(\"train/\"+fig,)\n",
    "        #img = cv2.cvtColor(src,cv2.COLOR_BGR2RGB)\n",
    "        img = image.load_img(\"train/\"+fig, target_size=(150, 150, 3))\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        X_train[count] = x\n",
    "        count += 1\n",
    "        if(count%1000==0):\n",
    "            print('images done :',count)\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing images\n",
      "images done : 1000\n",
      "images done : 2000\n",
      "images done : 3000\n",
      "images done : 4000\n",
      "images done : 5000\n",
      "images done : 6000\n",
      "images done : 7000\n",
      "images done : 8000\n",
      "images done : 9000\n",
      "images done : 10000\n",
      "images done : 11000\n",
      "images done : 12000\n",
      "images done : 13000\n",
      "images done : 14000\n",
      "images done : 15000\n",
      "images done : 16000\n",
      "images done : 17000\n"
     ]
    }
   ],
   "source": [
    "X_train = prepareImages(y_train,y_train.shape[0])\n",
    "X_train /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17034, 150, 150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "random_seed = 7\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Output into one hot code\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1 and as this is a multi classification problem so we can convert the output class values into one-hot format which is simply a binary matrix, i.e.\n",
    "\n",
    "value 0 will be converted to one-hot format as [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "value 1 will be converted to one-hot format as [0, 1, 0, 0, 0, 0, 0, 0, 0] etc\n",
    "\n",
    "Here we have {'buildings' -> 0, 'forest' -> 1, 'glacier' -> 2, 'mountain' -> 3, 'sea' -> 4, 'street' -> 5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode outputs'\n",
    "y_train_label = np_utils.to_categorical(y_train['label'])\n",
    "num_classes = y_train_label.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(150, 150, 3), activation='relu',data_format='channels_first'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = 4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "We will use data augmentation for increasing our dataset and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "- Define the model.\n",
    "- We will split the dataset using sklearn train_test_split into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 148, 1)        43232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 148, 1)        4         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 146, 64)       640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 146, 64)       256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 71, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 71, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 69, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 11, 69, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 34, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 34, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 31, 256)        524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 31, 256)        1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15872)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 95238     \n",
      "=================================================================\n",
      "Total params: 776,490\n",
      "Trainable params: 775,464\n",
      "Non-trainable params: 1,026\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "532/532 [==============================] - 140s 263ms/step - loss: 0.5328 - acc: 0.8323 - val_loss: 0.3838 - val_acc: 0.8615\n",
      "Epoch 2/70\n",
      "532/532 [==============================] - 110s 206ms/step - loss: 0.3282 - acc: 0.8676 - val_loss: 0.3092 - val_acc: 0.8665\n",
      "Epoch 3/70\n",
      "532/532 [==============================] - 148s 278ms/step - loss: 0.2855 - acc: 0.8819 - val_loss: 0.2928 - val_acc: 0.8760\n",
      "Epoch 4/70\n",
      "532/532 [==============================] - 126s 237ms/step - loss: 0.2639 - acc: 0.8911 - val_loss: 0.2707 - val_acc: 0.8867\n",
      "Epoch 5/70\n",
      "532/532 [==============================] - 114s 215ms/step - loss: 0.2556 - acc: 0.8958 - val_loss: 0.2905 - val_acc: 0.8779\n",
      "Epoch 6/70\n",
      "532/532 [==============================] - 125s 234ms/step - loss: 0.2612 - acc: 0.8916 - val_loss: 0.2814 - val_acc: 0.8851\n",
      "Epoch 7/70\n",
      "532/532 [==============================] - 113s 212ms/step - loss: 0.2438 - acc: 0.8981 - val_loss: 0.3236 - val_acc: 0.8702\n",
      "Epoch 8/70\n",
      "532/532 [==============================] - 111s 209ms/step - loss: 0.2413 - acc: 0.9007 - val_loss: 0.2550 - val_acc: 0.8923\n",
      "Epoch 9/70\n",
      "532/532 [==============================] - 142s 268ms/step - loss: 0.2387 - acc: 0.9019 - val_loss: 0.2486 - val_acc: 0.8932\n",
      "Epoch 10/70\n",
      "532/532 [==============================] - 120s 226ms/step - loss: 0.2264 - acc: 0.9053 - val_loss: 0.2271 - val_acc: 0.9012\n",
      "Epoch 11/70\n",
      "532/532 [==============================] - 114s 215ms/step - loss: 0.2179 - acc: 0.9102 - val_loss: 0.2247 - val_acc: 0.9048\n",
      "Epoch 12/70\n",
      "532/532 [==============================] - 177s 332ms/step - loss: 0.2160 - acc: 0.9100 - val_loss: 0.2161 - val_acc: 0.9091\n",
      "Epoch 13/70\n",
      "532/532 [==============================] - 157s 295ms/step - loss: 0.2146 - acc: 0.9105 - val_loss: 0.2305 - val_acc: 0.9049\n",
      "Epoch 14/70\n",
      "532/532 [==============================] - 120s 226ms/step - loss: 0.2075 - acc: 0.9128 - val_loss: 0.2099 - val_acc: 0.9112\n",
      "Epoch 15/70\n",
      "532/532 [==============================] - 113s 212ms/step - loss: 0.2019 - acc: 0.9157 - val_loss: 0.2060 - val_acc: 0.9121\n",
      "Epoch 16/70\n",
      "532/532 [==============================] - 127s 238ms/step - loss: 0.1957 - acc: 0.9179 - val_loss: 0.2120 - val_acc: 0.9115\n",
      "Epoch 17/70\n",
      "532/532 [==============================] - 125s 235ms/step - loss: 0.1963 - acc: 0.9181 - val_loss: 0.2022 - val_acc: 0.9164\n",
      "Epoch 18/70\n",
      "532/532 [==============================] - 112s 211ms/step - loss: 0.1876 - acc: 0.9218 - val_loss: 0.2045 - val_acc: 0.9121\n",
      "Epoch 19/70\n",
      "532/532 [==============================] - 108s 202ms/step - loss: 0.1828 - acc: 0.9246 - val_loss: 0.2061 - val_acc: 0.9123\n",
      "Epoch 20/70\n",
      "532/532 [==============================] - 118s 223ms/step - loss: 0.1807 - acc: 0.9257 - val_loss: 0.1933 - val_acc: 0.9182\n",
      "Epoch 21/70\n",
      "532/532 [==============================] - 138s 259ms/step - loss: 0.1783 - acc: 0.9267 - val_loss: 0.1915 - val_acc: 0.9213\n",
      "Epoch 22/70\n",
      "532/532 [==============================] - 140s 262ms/step - loss: 0.1758 - acc: 0.9272 - val_loss: 0.1888 - val_acc: 0.9217\n",
      "Epoch 23/70\n",
      "532/532 [==============================] - 122s 229ms/step - loss: 0.1736 - acc: 0.9271 - val_loss: 0.1886 - val_acc: 0.9219\n",
      "Epoch 24/70\n",
      "532/532 [==============================] - 110s 207ms/step - loss: 0.1664 - acc: 0.9310 - val_loss: 0.1956 - val_acc: 0.9184\n",
      "Epoch 25/70\n",
      "532/532 [==============================] - 117s 219ms/step - loss: 0.1717 - acc: 0.9294 - val_loss: 0.1826 - val_acc: 0.9232\n",
      "Epoch 26/70\n",
      "532/532 [==============================] - 113s 212ms/step - loss: 0.1651 - acc: 0.9319 - val_loss: 0.2031 - val_acc: 0.9161\n",
      "Epoch 27/70\n",
      "532/532 [==============================] - 109s 204ms/step - loss: 0.1628 - acc: 0.9333 - val_loss: 0.1851 - val_acc: 0.9228\n",
      "Epoch 28/70\n",
      "532/532 [==============================] - 113s 212ms/step - loss: 0.1611 - acc: 0.9342 - val_loss: 0.1880 - val_acc: 0.9223\n",
      "Epoch 29/70\n",
      "532/532 [==============================] - 105s 198ms/step - loss: 0.1603 - acc: 0.9334 - val_loss: 0.1863 - val_acc: 0.9243\n",
      "Epoch 30/70\n",
      "532/532 [==============================] - 119s 223ms/step - loss: 0.1568 - acc: 0.9360 - val_loss: 0.1848 - val_acc: 0.9245\n",
      "Epoch 31/70\n",
      "532/532 [==============================] - 121s 227ms/step - loss: 0.1547 - acc: 0.9371 - val_loss: 0.1822 - val_acc: 0.9249\n",
      "Epoch 32/70\n",
      "532/532 [==============================] - 125s 236ms/step - loss: 0.1547 - acc: 0.9362 - val_loss: 0.1862 - val_acc: 0.9237\n",
      "Epoch 33/70\n",
      "532/532 [==============================] - 109s 205ms/step - loss: 0.1506 - acc: 0.9377 - val_loss: 0.1825 - val_acc: 0.9257\n",
      "Epoch 34/70\n",
      "532/532 [==============================] - 109s 205ms/step - loss: 0.1495 - acc: 0.9390 - val_loss: 0.1851 - val_acc: 0.9242\n",
      "Epoch 35/70\n",
      "532/532 [==============================] - 119s 224ms/step - loss: 0.1490 - acc: 0.9381 - val_loss: 0.1824 - val_acc: 0.9249\n",
      "Epoch 36/70\n",
      "532/532 [==============================] - 120s 225ms/step - loss: 0.1454 - acc: 0.9402 - val_loss: 0.1768 - val_acc: 0.9275\n",
      "Epoch 37/70\n",
      "532/532 [==============================] - 134s 252ms/step - loss: 0.1464 - acc: 0.9404 - val_loss: 0.1795 - val_acc: 0.9271\n",
      "Epoch 38/70\n",
      "532/532 [==============================] - 175s 329ms/step - loss: 0.1422 - acc: 0.9415 - val_loss: 0.1771 - val_acc: 0.9309\n",
      "Epoch 39/70\n",
      "532/532 [==============================] - 329s 619ms/step - loss: 0.1430 - acc: 0.9413 - val_loss: 0.1767 - val_acc: 0.9280\n",
      "Epoch 40/70\n",
      "532/532 [==============================] - 179s 337ms/step - loss: 0.1415 - acc: 0.9425 - val_loss: 0.1780 - val_acc: 0.9290\n",
      "Epoch 41/70\n",
      "532/532 [==============================] - 283s 531ms/step - loss: 0.1407 - acc: 0.9426 - val_loss: 0.1802 - val_acc: 0.9283\n",
      "Epoch 42/70\n",
      "532/532 [==============================] - 181s 340ms/step - loss: 0.1387 - acc: 0.9434 - val_loss: 0.1805 - val_acc: 0.9264\n",
      "Epoch 43/70\n",
      "532/532 [==============================] - 200s 375ms/step - loss: 0.1362 - acc: 0.9453 - val_loss: 0.1797 - val_acc: 0.9285\n",
      "Epoch 44/70\n",
      "532/532 [==============================] - 184s 346ms/step - loss: 0.1378 - acc: 0.9434 - val_loss: 0.1814 - val_acc: 0.9275\n",
      "Epoch 45/70\n",
      "532/532 [==============================] - 118s 222ms/step - loss: 0.1347 - acc: 0.9449 - val_loss: 0.1833 - val_acc: 0.9280\n",
      "Epoch 46/70\n",
      "532/532 [==============================] - 123s 232ms/step - loss: 0.1345 - acc: 0.9459 - val_loss: 0.1787 - val_acc: 0.9298\n",
      "Epoch 47/70\n",
      "532/532 [==============================] - 126s 236ms/step - loss: 0.1352 - acc: 0.9451 - val_loss: 0.1821 - val_acc: 0.9275\n",
      "Epoch 48/70\n",
      "532/532 [==============================] - 113s 213ms/step - loss: 0.1323 - acc: 0.9466 - val_loss: 0.1803 - val_acc: 0.9271\n",
      "Epoch 49/70\n",
      "532/532 [==============================] - 110s 207ms/step - loss: 0.1323 - acc: 0.9461 - val_loss: 0.1782 - val_acc: 0.9287\n",
      "Epoch 50/70\n",
      "532/532 [==============================] - 113s 212ms/step - loss: 0.1293 - acc: 0.9482 - val_loss: 0.1828 - val_acc: 0.9267\n",
      "Epoch 51/70\n",
      "532/532 [==============================] - 113s 213ms/step - loss: 0.1284 - acc: 0.9478 - val_loss: 0.1811 - val_acc: 0.9275\n",
      "Epoch 52/70\n",
      "532/532 [==============================] - 116s 218ms/step - loss: 0.1302 - acc: 0.9477 - val_loss: 0.1764 - val_acc: 0.9288\n",
      "Epoch 53/70\n",
      "532/532 [==============================] - 119s 224ms/step - loss: 0.1264 - acc: 0.9492 - val_loss: 0.1746 - val_acc: 0.9308\n",
      "Epoch 54/70\n",
      "532/532 [==============================] - 132s 249ms/step - loss: 0.1277 - acc: 0.9490 - val_loss: 0.1797 - val_acc: 0.9284\n",
      "Epoch 55/70\n",
      "532/532 [==============================] - 115s 216ms/step - loss: 0.1283 - acc: 0.9481 - val_loss: 0.1778 - val_acc: 0.9280\n",
      "Epoch 56/70\n",
      "532/532 [==============================] - 111s 209ms/step - loss: 0.1255 - acc: 0.9486 - val_loss: 0.1772 - val_acc: 0.9292\n",
      "Epoch 57/70\n",
      "532/532 [==============================] - 125s 235ms/step - loss: 0.1277 - acc: 0.9480 - val_loss: 0.1762 - val_acc: 0.9297\n",
      "Epoch 58/70\n",
      "532/532 [==============================] - 110s 206ms/step - loss: 0.1241 - acc: 0.9491 - val_loss: 0.1794 - val_acc: 0.9276\n",
      "Epoch 59/70\n",
      "532/532 [==============================] - 122s 230ms/step - loss: 0.1239 - acc: 0.9501 - val_loss: 0.1803 - val_acc: 0.9260\n",
      "Epoch 60/70\n",
      "532/532 [==============================] - 147s 277ms/step - loss: 0.1266 - acc: 0.9491 - val_loss: 0.1767 - val_acc: 0.9308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/70\n",
      "532/532 [==============================] - 141s 265ms/step - loss: 0.1223 - acc: 0.9503 - val_loss: 0.1804 - val_acc: 0.9270\n",
      "Epoch 62/70\n",
      "532/532 [==============================] - 134s 253ms/step - loss: 0.1218 - acc: 0.9509 - val_loss: 0.1788 - val_acc: 0.9281\n",
      "Epoch 63/70\n",
      "532/532 [==============================] - 123s 231ms/step - loss: 0.1251 - acc: 0.9492 - val_loss: 0.1786 - val_acc: 0.9276\n",
      "Epoch 64/70\n",
      "532/532 [==============================] - 122s 230ms/step - loss: 0.1226 - acc: 0.9503 - val_loss: 0.1768 - val_acc: 0.9302\n",
      "Epoch 65/70\n",
      "532/532 [==============================] - 128s 241ms/step - loss: 0.1232 - acc: 0.9506 - val_loss: 0.1779 - val_acc: 0.9292\n",
      "Epoch 66/70\n",
      "532/532 [==============================] - 122s 229ms/step - loss: 0.1207 - acc: 0.9521 - val_loss: 0.1819 - val_acc: 0.9269\n",
      "Epoch 67/70\n",
      "532/532 [==============================] - 119s 224ms/step - loss: 0.1203 - acc: 0.9509 - val_loss: 0.1781 - val_acc: 0.9277\n",
      "Epoch 68/70\n",
      "532/532 [==============================] - 123s 231ms/step - loss: 0.1209 - acc: 0.9513 - val_loss: 0.1813 - val_acc: 0.9275\n",
      "Epoch 69/70\n",
      "532/532 [==============================] - 125s 235ms/step - loss: 0.1237 - acc: 0.9496 - val_loss: 0.1772 - val_acc: 0.9303\n",
      "Epoch 70/70\n",
      "532/532 [==============================] - 119s 224ms/step - loss: 0.1183 - acc: 0.9529 - val_loss: 0.1793 - val_acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "epochs = 70\n",
    "\n",
    "x_train2, x_val, y_train2, y_val = train_test_split(X_train, y_train_label, test_size = 0.10, random_state=random_seed)\n",
    "history = model1.fit_generator(datagen.flow(x_train2,y_train2, batch_size=32),\n",
    "                              epochs = epochs, validation_data = (x_val,y_val),\n",
    "                              verbose = 1, steps_per_epoch=(len(X_train)//32),validation_steps=(len(x_val)//32),callbacks=[annealer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "Increase data using more data augmentation and also use binary__crossentropy in place of categorical for this multi label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights('model1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Prepare the test Images and make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing images\n",
      "images done : 1000\n",
      "images done : 2000\n",
      "images done : 3000\n",
      "images done : 4000\n",
      "images done : 5000\n",
      "images done : 6000\n",
      "images done : 7000\n"
     ]
    }
   ],
   "source": [
    "X_test = prepareImages(y_test,y_test.shape[0])\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((X_test.shape[0],1))\n",
    "result = model1.predict(X_test)\n",
    "result = np.argmax(result,axis=1)\n",
    "result = pd.Series(result,name='label')\n",
    "submission = pd.concat([y_test,result],axis = 1)\n",
    "submission.to_csv(\"first_try.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
